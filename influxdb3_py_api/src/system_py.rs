// This is allowed because of the code generated by PyO3 for the PyResult type used
// in functions in this module.
#![allow(clippy::useless_conversion)]

use crate::ExecutePluginError;
use crate::logging::{LogLevel, ProcessingEngineLog};
use crate::system_py::CacheId::{Global, GlobalTest, Trigger, TriggerTest};
use anyhow::{Context, bail};
use arrow_array::types::Int32Type;
use arrow_array::{
    Array, BooleanArray, DictionaryArray, Float64Array, Int32Array, Int64Array, RecordBatch,
    StringArray, TimestampNanosecondArray, UInt64Array,
};
use arrow_schema::DataType;
use bytes::Bytes;
use chrono::{DateTime, Utc};
use futures::TryStreamExt;
use hashbrown::{HashMap, HashSet};
use humantime::format_duration;
use influxdb3_catalog::catalog::DatabaseSchema;
use influxdb3_id::TableId;
use influxdb3_internal_api::query_executor::QueryExecutor;
use influxdb3_sys_events::SysEventStore;
use influxdb3_wal::{FieldData, WriteBatch};
use iox_query_params::StatementParams;
use iox_time::{Time, TimeProvider};
use observability_deps::tracing::{error, info, warn};
use parking_lot::Mutex;
use pyo3::exceptions::{PyException, PyValueError};
use pyo3::prelude::{PyAnyMethods, PyModule};
use pyo3::types::{PyBytes, PyDateTime, PyDict, PyList, PyTuple};
use pyo3::{
    Bound, IntoPyObject, Py, PyAny, PyObject, PyResult, Python, create_exception, pyclass,
    pymethods, pymodule,
};
use std::collections::BTreeMap;
use std::ffi::CString;
use std::sync::Arc;
use std::time::Duration;

create_exception!(influxdb3_py_api, QueryError, PyException);

#[pyclass]
#[derive(Debug)]
struct PyPluginCallApi {
    db_schema: Arc<DatabaseSchema>,
    query_executor: Arc<dyn QueryExecutor>,
    return_state: Arc<Mutex<PluginReturnState>>,
    logger: Option<ProcessingEngineLogger>,
    py_cache: PyCache,
}

#[derive(Debug)]
pub enum CacheType {
    TestCache(String),
    Trigger {
        database: String,
        trigger_name: String,
    },
}

#[derive(Debug, Clone)]
pub struct ProcessingEngineLogger {
    sys_event_store: Arc<SysEventStore>,
    trigger_name: Arc<str>,
}

impl ProcessingEngineLogger {
    pub fn new(sys_event_store: Arc<SysEventStore>, trigger_name: impl Into<Arc<str>>) -> Self {
        Self {
            sys_event_store,
            trigger_name: trigger_name.into(),
        }
    }

    pub fn log(&self, log_level: LogLevel, log_line: impl Into<String>) {
        self.sys_event_store.record(ProcessingEngineLog::new(
            self.sys_event_store.time_provider().now(),
            log_level,
            Arc::clone(&self.trigger_name),
            log_line.into(),
        ))
    }
}

#[derive(Debug, Default)]
pub struct PluginReturnState {
    pub log_lines: Vec<LogLine>,
    pub write_back_lines: Vec<String>,
    pub write_db_lines: HashMap<String, Vec<String>>,
}

impl PluginReturnState {
    pub fn log(&self) -> Vec<String> {
        self.log_lines.iter().map(|l| l.to_string()).collect()
    }
}

pub enum LogLine {
    Info(String),
    Warn(String),
    Error(String),
}

impl std::fmt::Display for LogLine {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            LogLine::Info(s) => write!(f, "INFO: {s}"),
            LogLine::Warn(s) => write!(f, "WARN: {s}"),
            LogLine::Error(s) => write!(f, "ERROR: {s}"),
        }
    }
}

impl std::fmt::Debug for LogLine {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        std::fmt::Display::fmt(self, f)
    }
}

#[pymethods]
impl PyPluginCallApi {
    #[pyo3(signature = (*args))]
    fn info(&self, args: &Bound<'_, PyTuple>) -> PyResult<()> {
        let line = self.log_args_to_string(args)?;

        info!("processing engine: {}", line);
        self.write_to_logger(LogLevel::Info, line.clone());
        self.return_state.lock().log_lines.push(LogLine::Info(line));
        Ok(())
    }

    #[pyo3(signature = (*args))]
    fn warn(&self, args: &Bound<'_, PyTuple>) -> PyResult<()> {
        let line = self.log_args_to_string(args)?;

        warn!("processing engine: {}", line);
        self.write_to_logger(LogLevel::Warn, line.clone());
        self.return_state
            .lock()
            .log_lines
            .push(LogLine::Warn(line.to_string()));
        Ok(())
    }

    #[pyo3(signature = (*args))]
    fn error(&self, args: &Bound<'_, PyTuple>) -> PyResult<()> {
        let line = self.log_args_to_string(args)?;

        error!("processing engine: {}", line);
        self.write_to_logger(LogLevel::Error, line.clone());
        self.return_state
            .lock()
            .log_lines
            .push(LogLine::Error(line.to_string()));
        Ok(())
    }

    fn log_args_to_string(&self, args: &Bound<'_, PyTuple>) -> PyResult<String> {
        let line = args
            .try_iter()?
            .map(|arg| arg?.str()?.extract::<String>())
            .collect::<Result<Vec<String>, _>>()?
            .join(" ");
        Ok(line)
    }

    fn write(&self, line_builder: &Bound<'_, PyAny>) -> PyResult<()> {
        // Get the built line from the LineBuilder object
        let line = line_builder.getattr("build")?.call0()?;
        let line_str = line.extract::<String>()?;

        // Add to write_back_lines
        self.return_state.lock().write_back_lines.push(line_str);

        Ok(())
    }

    fn write_to_db(&self, db_name: &str, line_builder: &Bound<'_, PyAny>) -> PyResult<()> {
        let line = line_builder.getattr("build")?.call0()?;
        let line_str = line.extract::<String>()?;

        self.return_state
            .lock()
            .write_db_lines
            .entry(db_name.to_string())
            .or_default()
            .push(line_str);

        Ok(())
    }

    #[pyo3(signature = (query, args=None))]
    fn query(
        &self,
        query: String,
        args: Option<std::collections::HashMap<String, String>>,
    ) -> PyResult<Py<PyList>> {
        let query_executor = Arc::clone(&self.query_executor);
        let db_schema_name = Arc::clone(&self.db_schema.name);

        let params = args.map(|args| {
            let mut params = StatementParams::new();
            for (key, value) in args {
                params.insert(key, value);
            }
            params
        });

        // Spawn the async task
        let handle = tokio::spawn(async move {
            let res = query_executor
                .query_sql(db_schema_name.as_ref(), &query, params, None, None)
                .await
                .map_err(|e| QueryError::new_err(format!("error: {e} executing query: {query}")))?;

            res.try_collect()
                .await
                .map_err(|e| QueryError::new_err(format!("error: {e} executing query: {query}")))
        });

        // Block the current thread until the async task completes
        let res =
            tokio::task::block_in_place(|| tokio::runtime::Handle::current().block_on(handle));

        let res = res.map_err(|e| QueryError::new_err(format!("join error: {e}")))?;

        let batches: Vec<RecordBatch> = res?;

        Python::with_gil(|py| {
            let mut rows: Vec<PyObject> = Vec::new();

            for batch in batches {
                let num_rows = batch.num_rows();
                let schema = batch.schema();

                for row_idx in 0..num_rows {
                    let row = PyDict::new(py);
                    for col_idx in 0..schema.fields().len() {
                        let field = schema.field(col_idx);
                        let field_name = field.name().as_str();

                        let array = batch.column(col_idx);

                        match array.data_type() {
                            DataType::Int64 => {
                                let array = array.as_any().downcast_ref::<Int64Array>().unwrap();
                                row.set_item(field_name, array.value(row_idx))?;
                            }
                            DataType::UInt64 => {
                                let array = array.as_any().downcast_ref::<UInt64Array>().unwrap();
                                row.set_item(field_name, array.value(row_idx))?;
                            }
                            DataType::Float64 => {
                                let array = array.as_any().downcast_ref::<Float64Array>().unwrap();
                                row.set_item(field_name, array.value(row_idx))?;
                            }
                            DataType::Utf8 => {
                                let array = array.as_any().downcast_ref::<StringArray>().unwrap();
                                row.set_item(field_name, array.value(row_idx))?;
                            }
                            DataType::Boolean => {
                                let array = array.as_any().downcast_ref::<BooleanArray>().unwrap();
                                row.set_item(field_name, array.value(row_idx))?;
                            }
                            DataType::Timestamp(_, _) => {
                                let array = array
                                    .as_any()
                                    .downcast_ref::<TimestampNanosecondArray>()
                                    .unwrap();
                                row.set_item(field_name, array.value(row_idx))?;
                            }
                            DataType::Dictionary(_, _) => {
                                let col = array
                                    .as_any()
                                    .downcast_ref::<DictionaryArray<Int32Type>>()
                                    .expect("unexpected datatype");

                                let keys = col
                                    .keys()
                                    .as_any()
                                    .downcast_ref::<Int32Array>()
                                    .expect("unexpected datatype");

                                let values = col.values();
                                let values = values
                                    .as_any()
                                    .downcast_ref::<StringArray>()
                                    .expect("unexpected datatype");

                                let val = values.value(keys.value(row_idx) as usize).to_string();
                                row.set_item(field_name, val)?;
                            }
                            _ => {
                                return Err(PyValueError::new_err(format!(
                                    "Unsupported data type: {:?}",
                                    array.data_type()
                                )));
                            }
                        }
                    }
                    rows.push(row.into());
                }
            }

            let list = PyList::new(py, rows)?.unbind();
            Ok(list)
        })
    }

    #[getter]
    fn cache(&self) -> PyResult<PyCache> {
        self.py_cache.cache_store.lock().cleanup();
        Ok(self.py_cache.clone())
    }
}

impl PyPluginCallApi {
    fn write_to_logger(&self, level: LogLevel, log_line: String) {
        if let Some(logger) = &self.logger {
            logger.log(level, log_line);
        }
    }
}

// constant for the process writes call site string
const PROCESS_WRITES_CALL_SITE: &str = "process_writes";

const PROCESS_SCHEDULED_CALL_SITE: &str = "process_scheduled_call";

const PROCESS_REQUEST_CALL_SITE: &str = "process_request";

const LINE_BUILDER_CODE: &str = r#"
from typing import Optional
from collections import OrderedDict

class InfluxDBError(Exception):
    """Base exception for InfluxDB-related errors"""
    pass

class InvalidMeasurementError(InfluxDBError):
    """Raised when measurement name is invalid"""
    pass

class InvalidKeyError(InfluxDBError):
    """Raised when a tag or field key is invalid"""
    pass

class InvalidLineError(InfluxDBError):
    """Raised when a line protocol string is invalid"""
    pass

class LineBuilder:
    def __init__(self, measurement: str):
        if ' ' in measurement:
            raise InvalidMeasurementError("Measurement name cannot contain spaces")
        self.measurement = measurement
        self.tags: OrderedDict[str, str] = OrderedDict()
        self.fields: OrderedDict[str, str] = OrderedDict()
        self._timestamp_ns: Optional[int] = None

    def _validate_key(self, key: str, key_type: str) -> None:
        """Validate that a key does not contain spaces, commas, or equals signs."""
        if not key:
            raise InvalidKeyError(f"{key_type} key cannot be empty")
        if ' ' in key:
            raise InvalidKeyError(f"{key_type} key '{key}' cannot contain spaces")
        if ',' in key:
            raise InvalidKeyError(f"{key_type} key '{key}' cannot contain commas")
        if '=' in key:
            raise InvalidKeyError(f"{key_type} key '{key}' cannot contain equals signs")

    def _escape_measurement(self, value: str) -> str:
        """Escape characters in measurement names according to line protocol."""
        return value.replace(',', '\\,').replace(' ', '\\ ')

    def _escape_tag_value(self, value: str) -> str:
        """Escape characters in tag values according to line protocol."""
        return value.replace('\\', '\\\\').replace(',', '\\,').replace('=', '\\=').replace(' ', '\\ ')

    def _escape_field_key(self, value: str) -> str:
        """Escape characters in field keys according to line protocol."""
        return value.replace('\\', '\\\\').replace(',', '\\,').replace('=', '\\=').replace(' ', '\\ ')

    def tag(self, key: str, value: str) -> 'LineBuilder':
        """Add a tag to the line protocol."""
        self._validate_key(key, "tag")
        self.tags[key] = str(value)
        return self

    def uint64_field(self, key: str, value: int) -> 'LineBuilder':
        """Add an unsigned integer field to the line protocol."""
        self._validate_key(key, "field")
        if value < 0:
            raise ValueError(f"uint64 field '{key}' cannot be negative")
        self.fields[key] = f"{value}u"
        return self

    def int64_field(self, key: str, value: int) -> 'LineBuilder':
        """Add an integer field to the line protocol."""
        self._validate_key(key, "field")
        self.fields[key] = f"{value}i"
        return self

    def float64_field(self, key: str, value: float) -> 'LineBuilder':
        """Add a float field to the line protocol."""
        self._validate_key(key, "field")
        # Check if value has no decimal component
        self.fields[key] = f"{int(value)}.0" if value % 1 == 0 else str(value)
        return self

    def string_field(self, key: str, value: str) -> 'LineBuilder':
        """Add a string field to the line protocol."""
        self._validate_key(key, "field")
        # Escape quotes and backslashes in string values
        escaped_value = value.replace('\\', '\\\\').replace('"', '\\"')
        self.fields[key] = f'"{escaped_value}"'
        return self

    def bool_field(self, key: str, value: bool) -> 'LineBuilder':
        """Add a boolean field to the line protocol."""
        self._validate_key(key, "field")
        self.fields[key] = 't' if value else 'f'
        return self

    def time_ns(self, timestamp_ns: int) -> 'LineBuilder':
        """Set the timestamp in nanoseconds."""
        self._timestamp_ns = timestamp_ns
        return self

    def build(self) -> str:
        """Build the line protocol string."""
        # Start with measurement name (escape commas and spaces)
        line = self._escape_measurement(self.measurement)

        # Add tags if present
        if self.tags:
            tags_str = ','.join(
                f"{key}={self._escape_tag_value(value)}"
                for key, value in self.tags.items()
            )
            line += f",{tags_str}"

        # Add fields (required)
        if not self.fields:
            raise InvalidLineError(f"At least one field is required: {line}")

        fields_str = ','.join(
            f"{self._escape_field_key(key)}={value}"
            for key, value in self.fields.items()
        )
        line += f" {fields_str}"

        # Add timestamp if present
        if self._timestamp_ns is not None:
            line += f" {self._timestamp_ns}"

        return line"#;

fn args_to_py_object<'py>(
    py: Python<'py>,
    args: &Option<HashMap<String, String>>,
) -> Option<Bound<'py, PyDict>> {
    args.as_ref().map(|args| map_to_py_object(py, args))
}

fn map_to_py_object<'py>(py: Python<'py>, map: &HashMap<String, String>) -> Bound<'py, PyDict> {
    let dict = PyDict::new(py);
    for (key, value) in map {
        dict.set_item(key, value).unwrap();
    }
    dict
}

#[allow(clippy::too_many_arguments)]
pub fn execute_python_with_batch(
    code: &str,
    write_batch: &WriteBatch,
    schema: Arc<DatabaseSchema>,
    query_executor: Arc<dyn QueryExecutor>,
    logger: Option<ProcessingEngineLogger>,
    table_filter: Option<TableId>,
    args: &Option<HashMap<String, String>>,
    py_cache: PyCache,
) -> Result<PluginReturnState, ExecutePluginError> {
    let start_time = if let Some(logger) = &logger {
        logger.log(
            LogLevel::Info,
            "starting execution of wal plugin.".to_string(),
        );
        Some(logger.sys_event_store.time_provider().now())
    } else {
        None
    };
    Python::with_gil(|py| {
        // import the LineBuilder for use in the python code

        py.run(&CString::new(LINE_BUILDER_CODE).unwrap(), None, None)
            .map_err(|e| {
                anyhow::Error::new(e).context("failed to eval the LineBuilder API code")
            })?;

        // convert the write batch into a python object
        let mut table_batches = Vec::with_capacity(write_batch.table_chunks.len());

        for (table_id, table_chunks) in &write_batch.table_chunks {
            if let Some(table_filter) = table_filter {
                if table_id != &table_filter {
                    continue;
                }
            }
            let table_def = schema
                .tables
                .get_by_id(table_id)
                .context("table not found")?;

            let dict = PyDict::new(py);
            dict.set_item("table_name", table_def.table_name.as_ref())
                .context("failed to set table_name")?;

            let mut rows: Vec<PyObject> = Vec::new();
            for chunk in table_chunks.chunk_time_to_chunk.values() {
                for row in &chunk.rows {
                    let py_row = PyDict::new(py);

                    for field in &row.fields {
                        let field_name = table_def
                            .column_id_to_name(&field.id)
                            .context("field not found")?;
                        match &field.value {
                            FieldData::String(s) => {
                                py_row
                                    .set_item(field_name.as_ref(), s.as_str())
                                    .context("failed to set string field")?;
                            }
                            FieldData::Integer(i) => {
                                py_row
                                    .set_item(field_name.as_ref(), i)
                                    .context("failed to set integer field")?;
                            }
                            FieldData::UInteger(u) => {
                                py_row
                                    .set_item(field_name.as_ref(), u)
                                    .context("failed to set unsigned integer field")?;
                            }
                            FieldData::Float(f) => {
                                py_row
                                    .set_item(field_name.as_ref(), f)
                                    .context("failed to set float field")?;
                            }
                            FieldData::Boolean(b) => {
                                py_row
                                    .set_item(field_name.as_ref(), b)
                                    .context("failed to set boolean field")?;
                            }
                            FieldData::Tag(t) => {
                                py_row
                                    .set_item(field_name.as_ref(), t.as_str())
                                    .context("failed to set tag field")?;
                            }
                            FieldData::Timestamp(t) => {
                                py_row
                                    .set_item(field_name.as_ref(), t)
                                    .context("failed to set timestamp")?;
                            }
                            FieldData::Key(_) => {
                                unreachable!("key type should never be constructed")
                            }
                        };
                    }

                    rows.push(py_row.into());
                }
            }

            let rows = PyList::new(py, rows).context("failed to create rows list")?;

            dict.set_item("rows", rows.unbind())
                .context("failed to set rows")?;
            table_batches.push(dict);
        }

        let py_batches =
            PyList::new(py, table_batches).context("failed to create table_batches list")?;

        let api = PyPluginCallApi {
            db_schema: schema,
            query_executor,
            logger: logger.clone(),
            return_state: Default::default(),
            py_cache,
        };
        let return_state = Arc::clone(&api.return_state);
        let local_api = api.into_pyobject(py).map_err(anyhow::Error::from)?;

        // turn args into an optional dict to pass into python
        let args = args_to_py_object(py, args);

        // run the code and get the python function to call
        py.run(&CString::new(code).unwrap(), None, None)
            .map_err(anyhow::Error::from)?;
        let py_func = py
            .eval(&CString::new(PROCESS_WRITES_CALL_SITE).unwrap(), None, None)
            .map_err(|_| ExecutePluginError::MissingProcessWritesFunction)?;

        py_func
            .call1((local_api, py_batches.unbind(), args))
            .map_err(anyhow::Error::from)?;

        // swap with an empty return state to avoid cloning
        let empty_return_state = PluginReturnState::default();
        let ret = std::mem::replace(&mut *return_state.lock(), empty_return_state);

        if let Some(logger) = &logger {
            let runtime = logger
                .sys_event_store
                .time_provider()
                .now()
                .checked_duration_since(start_time.unwrap());
            logger.log(
                LogLevel::Info,
                format!(
                    "finished execution in {}",
                    format_duration(runtime.unwrap_or_default())
                ),
            );
        }

        Ok(ret)
    })
}

pub fn execute_schedule_trigger(
    code: &str,
    schedule_time: DateTime<Utc>,
    schema: Arc<DatabaseSchema>,
    query_executor: Arc<dyn QueryExecutor>,
    logger: Option<ProcessingEngineLogger>,
    args: &Option<HashMap<String, String>>,
    py_cache: PyCache,
) -> Result<PluginReturnState, ExecutePluginError> {
    let start_time = if let Some(logger) = &logger {
        logger.log(
            LogLevel::Info,
            format!("starting execution with scheduled time {schedule_time}"),
        );
        Some(logger.sys_event_store.time_provider().now())
    } else {
        None
    };
    Python::with_gil(|py| {
        // import the LineBuilder for use in the python code

        let py_datetime = PyDateTime::from_timestamp(py, schedule_time.timestamp() as f64, None)
            .map_err(|e| {
                anyhow::Error::new(e).context("error converting the schedule time to Python time")
            })?;

        py.run(&CString::new(LINE_BUILDER_CODE).unwrap(), None, None)
            .map_err(|e| {
                anyhow::Error::new(e).context("failed to eval the LineBuilder API code")
            })?;

        let api = PyPluginCallApi {
            db_schema: schema,
            query_executor,
            logger: logger.clone(),
            return_state: Default::default(),
            py_cache,
        };
        let return_state = Arc::clone(&api.return_state);
        let local_api = api.into_pyobject(py).map_err(anyhow::Error::from)?;

        // turn args into an optional dict to pass into python
        let args = args_to_py_object(py, args);

        // run the code and get the python function to call
        py.run(&CString::new(code).unwrap(), None, None)
            .map_err(anyhow::Error::from)?;

        let py_func = py
            .eval(
                &CString::new(PROCESS_SCHEDULED_CALL_SITE).unwrap(),
                None,
                None,
            )
            .map_err(|_| ExecutePluginError::MissingProcessScheduledCallFunction)?;

        py_func
            .call1((local_api, py_datetime, args))
            .map_err(anyhow::Error::from)?;

        // swap with an empty return state to avoid cloning
        let empty_return_state = PluginReturnState::default();
        let ret = std::mem::replace(&mut *return_state.lock(), empty_return_state);
        if let Some(logger) = &logger {
            let runtime = logger
                .sys_event_store
                .time_provider()
                .now()
                .checked_duration_since(start_time.unwrap());
            logger.log(
                LogLevel::Info,
                format!(
                    "finished execution in {}",
                    format_duration(runtime.unwrap_or_default())
                ),
            );
        }
        Ok(ret)
    })
}

#[allow(clippy::too_many_arguments)]
pub fn execute_request_trigger(
    code: &str,
    db_schema: Arc<DatabaseSchema>,
    query_executor: Arc<dyn QueryExecutor>,
    logger: Option<ProcessingEngineLogger>,
    args: &Option<HashMap<String, String>>,
    query_params: HashMap<String, String>,
    request_headers: HashMap<String, String>,
    request_body: Bytes,
    py_cache: PyCache,
) -> Result<(u16, HashMap<String, String>, String, PluginReturnState), ExecutePluginError> {
    let start_time = if let Some(logger) = &logger {
        logger.log(
            LogLevel::Info,
            "starting execution of request plugin.".to_string(),
        );
        Some(logger.sys_event_store.time_provider().now())
    } else {
        None
    };
    Python::with_gil(|py| {
        // import the LineBuilder for use in the python code
        py.run(&CString::new(LINE_BUILDER_CODE).unwrap(), None, None)
            .map_err(|e| {
                anyhow::Error::new(e).context("failed to eval the LineBuilder API code")
            })?;

        let api = PyPluginCallApi {
            db_schema,
            query_executor,
            logger: logger.clone(),
            return_state: Default::default(),
            py_cache,
        };
        let return_state = Arc::clone(&api.return_state);
        let local_api = api.into_pyobject(py).map_err(anyhow::Error::from)?;

        // turn args into an optional dict to pass into python
        let args = args_to_py_object(py, args);

        let query_params = map_to_py_object(py, &query_params);
        let request_params = map_to_py_object(py, &request_headers);

        // run the code and get the python function to call
        py.run(&CString::new(code).unwrap(), None, None)
            .map_err(anyhow::Error::from)?;

        let py_func = py
            .eval(
                &CString::new(PROCESS_REQUEST_CALL_SITE).unwrap(),
                None,
                None,
            )
            .map_err(|_| ExecutePluginError::MissingProcessRequestFunction)?;

        // convert the body bytes into python bytes blob
        let request_body = PyBytes::new(py, &request_body[..]);

        // get the result from calling the python function
        let result = py_func
            .call1((local_api, query_params, request_params, request_body, args))
            .map_err(|e| anyhow::anyhow!("Python function call failed: {}", e))?;

        // Process the result according to Flask conventions
        let (response_code, response_headers, response_body) = process_flask_response(py, result)?;

        // swap with an empty return state to avoid cloning
        let empty_return_state = PluginReturnState::default();
        let ret = std::mem::replace(&mut *return_state.lock(), empty_return_state);

        if let Some(logger) = &logger {
            let runtime = logger
                .sys_event_store
                .time_provider()
                .now()
                .checked_duration_since(start_time.unwrap());
            logger.log(
                LogLevel::Info,
                format!(
                    "finished execution in {}",
                    format_duration(runtime.unwrap_or_default())
                ),
            )
        }

        Ok((response_code, response_headers, response_body, ret))
    })
}

fn process_flask_response(
    py: Python<'_>,
    result: Bound<'_, PyAny>,
) -> Result<(u16, HashMap<String, String>, String), anyhow::Error> {
    let default_status: u16 = 200;
    let default_headers: HashMap<String, String> = HashMap::new();
    let default_mimetype = "text/html";

    // Check if it's a Flask Response object
    if let Ok(true) = result
        .call_method0("__flask_response__")
        .and_then(|r| r.extract::<bool>())
    {
        // It's a Flask Response object, extract status, headers, and body
        let status: u16 = result.getattr("status_code")?.extract()?;
        let body: String = result.call_method0("get_data")?.extract()?;

        // Extract headers
        let headers_dict = result.getattr("headers")?;
        let headers: std::collections::HashMap<String, String> = headers_dict.extract()?;

        return Ok((status, headers.into_iter().collect(), body));
    }

    // Check if it's a tuple
    if let Ok(tuple) = result.downcast::<PyTuple>() {
        let tuple_len = tuple.len()?;
        if tuple_len > 0 && tuple_len <= 3 {
            // Extract response part (first element)
            let response = tuple.get_item(0)?;
            let (response_body, mut headers) = process_response_part(py, response)?;

            let mut status = default_status;

            // Handle status and/or headers if provided
            if tuple_len >= 2 {
                let second = tuple.get_item(1)?;

                // Check if the second item is a status code or headers
                if let Ok(status_code) = second.extract::<i64>() {
                    status = status_code.try_into().unwrap_or(500);
                } else if let Ok(header_dict) = second.downcast::<PyDict>() {
                    // It's headers
                    let additional_headers: std::collections::HashMap<String, String> =
                        header_dict.extract()?;
                    headers.extend(additional_headers);
                }

                // If there's a third item, it must be headers
                if tuple_len == 3 {
                    let header_item = tuple.get_item(2)?;
                    let Ok(header_dict) = header_item.downcast::<PyDict>() else {
                        bail!("expected a dictionary in header item");
                    };
                    let additional_headers: std::collections::HashMap<String, String> =
                        header_dict.extract()?;
                    headers.extend(additional_headers);
                }
            }

            return Ok((status, headers, response_body));
        }
    }

    // Check if it's a string
    if let Ok(string_val) = result.extract::<String>() {
        let mut headers = default_headers.clone();
        headers.insert("Content-Type".to_string(), default_mimetype.to_string());
        return Ok((default_status, headers, string_val));
    }

    // Check if it's a dict or list for JSON response
    if result.is_instance_of::<PyDict>() || result.is_instance_of::<PyList>() {
        // We need to jsonify this
        let json_module = py.import("json")?;
        let json_string = json_module.call_method1("dumps", (result,))?;
        let response_body: String = json_string.extract()?;

        let mut headers = default_headers.clone();
        headers.insert("Content-Type".to_string(), "application/json".to_string());

        return Ok((default_status, headers, response_body));
    }

    // Check if it's an iterator or generator returning strings or bytes
    if let Ok(true) = result.hasattr("__iter__") {
        // Handling streaming response would require adapting the return type
        // For now, we'll convert it to a concatenated string
        let mut stream_content = String::new();

        let iter = result.try_iter()?;
        for item in iter {
            let item = item?;
            if let Ok(s) = item.extract::<String>() {
                stream_content.push_str(&s);
            } else if let Ok(b) = item.extract::<&[u8]>() {
                // Convert bytes to string - might need better UTF-8 handling
                if let Ok(s) = std::str::from_utf8(b) {
                    stream_content.push_str(s);
                }
            }
        }

        let mut headers = default_headers.clone();
        headers.insert("Content-Type".to_string(), default_mimetype.to_string());

        return Ok((default_status, headers, stream_content));
    }

    // If we can't identify the response type, return an error
    Err(anyhow::anyhow!(
        "Unsupported return type from Python function"
    ))
}

fn process_response_part(
    py: Python<'_>,
    response: Bound<'_, PyAny>,
) -> Result<(String, HashMap<String, String>), anyhow::Error> {
    let mut headers = HashMap::new();

    // If it's a string, return it directly
    if let Ok(string_val) = response.extract::<String>() {
        headers.insert("Content-Type".to_string(), "text/html".to_string());
        return Ok((string_val, headers));
    }

    // If it's a dict or list, convert to JSON
    if response.is_instance_of::<PyDict>() || response.is_instance_of::<PyList>() {
        let json_module = py.import("json")?;
        let json_string = json_module.call_method1("dumps", (response,))?;
        let response_body: String = json_string.extract()?;

        headers.insert("Content-Type".to_string(), "application/json".to_string());
        return Ok((response_body, headers));
    }

    // Default fallback
    let response_str = response.str()?.extract::<String>()?;
    headers.insert("Content-Type".to_string(), "text/plain".to_string());

    Ok((response_str, headers))
}

// Cache entry with optional expiration
#[derive(Debug)]
pub struct CacheEntry {
    value: Py<PyAny>,         // Python object reference with proper reference counting
    expires_at: Option<Time>, // Expiration time if any
}

impl CacheEntry {
    fn new(value: Py<PyAny>, ttl: Option<f64>, time_provider: Arc<dyn TimeProvider>) -> Self {
        let expires_at = ttl.map(|seconds| time_provider.now() + Duration::from_secs_f64(seconds));
        Self { value, expires_at }
    }
}

// Cache store that manages namespaced caches
#[derive(Debug)]
pub struct CacheStore {
    // Map of namespace -> (key -> cache entry)
    namespaces: HashMap<CacheId, ExpiringCache>,
    time_provider: Arc<dyn TimeProvider>,
    last_cleanup: Time,
    cleanup_interval: Duration,
}

#[derive(Debug)]
pub struct ExpiringCache {
    entries: HashMap<String, CacheEntry>,
    expirations: BTreeMap<Time, HashSet<String>>,
    default_ttl: Option<Duration>,
    time_provider: Arc<dyn TimeProvider>,
}

impl ExpiringCache {
    fn new(time_provider: Arc<dyn TimeProvider>, default_ttl: Option<Duration>) -> Self {
        Self {
            entries: HashMap::default(),
            expirations: BTreeMap::default(),
            default_ttl,
            time_provider,
        }
    }

    fn insert(&mut self, key: String, mut entry: CacheEntry) {
        // this is for test call caches, which should expire.
        if self.default_ttl.is_some() && entry.expires_at.is_none() {
            entry.expires_at = Some(self.time_provider.now() + self.default_ttl.unwrap());
        }
        // if key has a ttl, record its expiration.
        let expiration = entry.expires_at;
        // if this was an overwrite, remove from expirations
        if let Some(old_entry) = self.entries.insert(key.clone(), entry) {
            if let Some(old_expiration) = old_entry.expires_at {
                self.expirations
                    .get_mut(&old_expiration)
                    .unwrap()
                    .remove(&key);
            }
        }
        if let Some(expiration) = expiration {
            self.expirations.entry(expiration).or_default().insert(key);
        }
    }

    fn get(&mut self, key: &str, py: Python<'_>) -> Option<Py<PyAny>> {
        let entry = self.entries.get(key)?;
        if let Some(expiration) = entry.expires_at {
            if expiration <= self.time_provider.now() {
                self.remove(key);
                return None;
            }
        }
        Some(entry.value.clone_ref(py))
    }

    fn remove(&mut self, key: &str) -> bool {
        if let Some(entry) = self.entries.remove(key) {
            if let Some(expiration) = entry.expires_at {
                self.expirations.get_mut(&expiration).unwrap().remove(key);
            }
            return true;
        }
        false
    }

    fn cleanup(&mut self) {
        let now = self.time_provider.now();
        if self.expirations.is_empty() || *self.expirations.first_key_value().unwrap().0 > now {
            return;
        }
        let mut unexpired_expirations = self.expirations.split_off(&now);
        for (_, keys) in self.expirations.iter() {
            for key in keys {
                self.entries.remove(key);
            }
        }
        std::mem::swap(&mut self.expirations, &mut unexpired_expirations);
    }

    fn is_empty(&self) -> bool {
        self.entries.is_empty()
    }
}

#[pyclass]
#[derive(Hash, Eq, PartialEq, Debug, Clone)]
enum CacheId {
    Global(),
    GlobalTest(String),
    Trigger {
        database: String,
        trigger_name: String,
    },
    TriggerTest(String),
}

impl CacheId {
    fn default_expiration(&self) -> Option<Duration> {
        match self {
            Global() | Trigger { .. } => None,
            // for tests, keep values around for 30 minutes.
            GlobalTest(_) | TriggerTest(_) => Some(Duration::from_secs(30 * 60)),
        }
    }
}

impl CacheStore {
    pub fn new(time_provider: Arc<dyn TimeProvider>, cleanup_interval: Duration) -> Self {
        let last_cleanup = time_provider.now();
        Self {
            namespaces: HashMap::new(),
            time_provider,
            cleanup_interval,
            last_cleanup,
        }
    }

    fn should_run_cleanup(&self) -> bool {
        self.time_provider
            .now()
            .checked_duration_since(self.last_cleanup)
            .unwrap_or_default()
            >= self.cleanup_interval
    }

    fn cleanup(&mut self) {
        if !self.should_run_cleanup() {
            return;
        }
        for cache in self.namespaces.values_mut() {
            cache.cleanup();
        }

        self.namespaces.retain(|_, cache| !cache.is_empty());
    }

    fn put(&mut self, cache_id: &CacheId, key: &str, value: Py<PyAny>, ttl: Option<f64>) {
        let entry = CacheEntry::new(value, ttl, Arc::clone(&self.time_provider));
        if let Some(cache) = self.namespaces.get_mut(cache_id) {
            cache.insert(key.to_string(), entry);
            return;
        }
        self.namespaces
            .entry(cache_id.clone())
            .or_insert_with(|| {
                ExpiringCache::new(
                    Arc::clone(&self.time_provider),
                    cache_id.default_expiration(),
                )
            })
            .insert(key.to_string(), entry);
    }

    fn get(&mut self, cache_id: &CacheId, py: Python<'_>, key: &str) -> Option<Py<PyAny>> {
        let cache = self.namespaces.get_mut(cache_id)?;

        cache.get(key, py)
    }

    fn delete(&mut self, cache_id: &CacheId, key: &str) -> bool {
        if let Some(cache) = self.namespaces.get_mut(cache_id) {
            cache.remove(key)
        } else {
            false
        }
    }
    pub fn drop_trigger_cache(&mut self, database: String, trigger_name: String) -> bool {
        self.namespaces
            .remove(&Trigger {
                database,
                trigger_name,
            })
            .is_some()
    }
}

// Python class for Cache
#[pyclass]
#[derive(Debug, Clone)]
pub struct PyCache {
    global_cache_id: CacheId,
    local_cache_id: CacheId,
    cache_store: Arc<Mutex<CacheStore>>,
}

impl PyCache {
    fn cache_id(&self, use_global: Option<bool>) -> &CacheId {
        if use_global.unwrap_or_default() {
            &self.global_cache_id
        } else {
            &self.local_cache_id
        }
    }
    pub fn new_test_cache(cache_store: Arc<Mutex<CacheStore>>, test_name: String) -> Self {
        Self {
            global_cache_id: GlobalTest(test_name.clone()),
            local_cache_id: TriggerTest(test_name),
            cache_store,
        }
    }

    pub fn new_trigger_cache(
        cache_store: Arc<Mutex<CacheStore>>,
        database: String,
        trigger_name: String,
    ) -> Self {
        Self {
            global_cache_id: Global(),
            local_cache_id: Trigger {
                database,
                trigger_name,
            },
            cache_store,
        }
    }
}

#[pymethods]
impl PyCache {
    #[pyo3(signature = (key, value, ttl=None, use_global=None))]
    fn put(
        &self,
        key: String,
        value: Py<PyAny>,
        ttl: Option<f64>,
        use_global: Option<bool>,
    ) -> PyResult<()> {
        let cache_id = self.cache_id(use_global);
        self.cache_store.lock().put(cache_id, &key, value, ttl);

        Ok(())
    }

    #[pyo3(signature = (key, default=None, use_global=None))]
    fn get(
        &self,
        key: String,
        default: Option<Py<PyAny>>,
        use_global: Option<bool>,
    ) -> PyResult<PyObject> {
        Python::with_gil(|py| {
            let cache_id = self.cache_id(use_global);
            let result = self.cache_store.lock().get(cache_id, py, &key);

            match result {
                Some(value) => {
                    // Return the retrieved Python object
                    Ok(value)
                }
                None => {
                    // Return the default value if provided
                    if let Some(default_value) = default {
                        Ok(default_value)
                    } else {
                        // Return None if no default was provided
                        let x = py.None();
                        Ok(x)
                    }
                }
            }
        })
    }

    #[pyo3(signature = (key, use_global=None))]
    fn delete(&self, key: String, use_global: Option<bool>) -> PyResult<bool> {
        let cache_id = self.cache_id(use_global);
        let result = self.cache_store.lock().delete(cache_id, &key);

        Ok(result)
    }
}

// Module initialization
#[pymodule]
fn influxdb3_py_api(_m: &Bound<'_, PyModule>) -> PyResult<()> {
    Ok(())
}
