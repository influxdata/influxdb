// This is allowed because of the code generated by PyO3 for the PyResult type used
// in functions in this module.
#![allow(clippy::useless_conversion)]

use crate::ExecutePluginError;
use crate::logging::{LogLevel, ProcessingEngineLog};
use crate::system_py::CacheId::{Global, GlobalTest, Trigger, TriggerTest};
use anyhow::{anyhow, bail};
use arrow_array::types::Int32Type;
use arrow_array::{
    Array, BooleanArray, DictionaryArray, Float64Array, Int64Array, RecordBatch, StringArray,
    TimestampNanosecondArray, UInt64Array,
};
use arrow_schema::DataType;
use bytes::Bytes;
use chrono::{DateTime, Utc};
use data_types::NamespaceName;
use futures::TryStreamExt;
use hashbrown::{HashMap, HashSet};
use humantime::format_duration;
use influxdb3_catalog::catalog::DatabaseSchema;
use influxdb3_id::TableId;
use influxdb3_internal_api::query_executor::QueryExecutor;
use influxdb3_sys_events::SysEventStore;
use influxdb3_write::{Bufferer, Precision};
use iox_query_params::StatementParams;
use iox_time::{Time, TimeProvider};
use observability_deps::tracing::{error, info, warn};
use parking_lot::Mutex;
use pyo3::exceptions::{PyException, PyValueError};
use pyo3::prelude::{PyAnyMethods, PyModule};
use pyo3::types::{PyBytes, PyDateTime, PyDict, PyList, PyString, PyTuple};
use pyo3::{
    Bound, IntoPyObject, Py, PyAny, PyResult, Python, create_exception, pyclass, pymethods,
    pymodule,
};
use std::collections::BTreeMap;
use std::ffi::CString;
use std::path::Path;
use std::sync::Arc;
use std::time::{Duration, Instant, SystemTime};

create_exception!(influxdb3_py_api, QueryError, PyException);

/// API object exposed to Python plugins via PyO3.
///
/// Methods on this struct are called from Python code. They may appear unused from
/// a Rust perspective but are invoked by the Python runtime when plugins call methods like
/// `influxdb3_local.write()`, `influxdb3_local.query()`, etc.
#[pyclass]
#[derive(Debug)]
struct PyPluginCallApi {
    db_schema: Arc<DatabaseSchema>,
    query_executor: Arc<dyn QueryExecutor>,
    write_buffer: Arc<dyn Bufferer>,
    write_accumulator: Arc<Mutex<WriteAccumulator>>,
    logger: Arc<PluginLogger>,
    py_cache: PyCache,
}

/// Accumulates batched write operations during plugin execution.
///
/// See [PluginReturnState].
#[derive(Debug, Default)]
struct WriteAccumulator {
    /// Line protocol to write back to the database owning the trigger.
    write_back_lines: Vec<String>,
    /// Line protocol to write to other databases, keyed by database name.
    write_db_lines: HashMap<String, Vec<String>>,
}

#[derive(Debug)]
pub enum CacheType {
    TestCache(String),
    Trigger {
        database: String,
        trigger_name: String,
    },
}

#[derive(Debug, Clone)]
pub struct ProcessingEngineLogger {
    sys_event_store: Arc<SysEventStore>,
    trigger_name: Arc<str>,
}

impl ProcessingEngineLogger {
    pub fn new(sys_event_store: Arc<SysEventStore>, trigger_name: impl Into<Arc<str>>) -> Self {
        Self {
            sys_event_store,
            trigger_name: trigger_name.into(),
        }
    }

    pub fn log(&self, log_level: LogLevel, log_line: impl Into<String>) {
        self.sys_event_store.record(ProcessingEngineLog::new(
            self.sys_event_store.time_provider().now(),
            log_level,
            Arc::clone(&self.trigger_name),
            log_line.into(),
        ))
    }
}

/// Accumulated state returned from plugin execution.
///
/// After a plugin finishes, this struct contains data that must be processed:
/// log messages to emit and line protocol to write.
#[derive(Debug, Default)]
pub struct PluginReturnState {
    /// Log messages emitted during plugin execution.
    pub log_lines: Vec<LogLine>,
    /// Line protocol to write back to the database owning the trigger.
    pub write_back_lines: Vec<String>,
    /// Line protocol to write to other databases, keyed by database name.
    pub write_db_lines: HashMap<String, Vec<String>>,
}

/// Logger abstraction for plugin execution.
///
/// In production mode, logs are written to the sys_event_store only.
/// In dry run mode, logs are accumulated for the response (no sys_event_store writes).
/// Tracing macros (info!, warn!, error!) are called in PyPluginCallApi methods for both modes.
#[derive(Debug)]
pub enum PluginLogger {
    /// Production mode: logs to sys_event_store only
    Production(ProcessingEngineLogger),
    /// Dry run mode: accumulates log_lines only.
    /// Note: Mutex is required for Send+Sync bounds (PyO3's #[macro@pyclass] requires Send),
    /// not for actual concurrent access - execution is single-threaded within the GIL.
    // todo(pjb): potential memory issue - unbounded log accumulation
    DryRun { log_lines: Mutex<Vec<LogLine>> },
}

impl PluginLogger {
    /// Create a production logger that writes to sys_event_store.
    pub fn production(logger: ProcessingEngineLogger) -> Self {
        Self::Production(logger)
    }

    /// Create a dry run logger that accumulates log lines in memory.
    pub fn dry_run() -> Self {
        Self::DryRun {
            log_lines: Mutex::new(Vec::new()),
        }
    }

    /// Log a message at the specified level.
    ///
    /// In production mode, writes to the sys_event_store for persistence.
    /// In dry run mode, accumulates the log line for later retrieval via `take_log_lines()`.
    pub fn log(&self, level: LogLevel, line: String) {
        match self {
            Self::Production(logger) => {
                logger.log(level, line);
            }
            Self::DryRun { log_lines } => {
                let log_line = match level {
                    LogLevel::Info => LogLine::Info(line),
                    LogLevel::Warn => LogLine::Warn(line),
                    LogLevel::Error => LogLine::Error(line),
                };
                log_lines.lock().push(log_line);
            }
        }
    }

    /// Take and return accumulated log lines.
    ///
    /// Returns an empty Vec for production loggers.
    /// Returns and clears the accumulated log lines for dry run loggers.
    pub fn take_log_lines(&self) -> Vec<LogLine> {
        match self {
            Self::Production(_) => Vec::new(),
            Self::DryRun { log_lines } => std::mem::take(&mut *log_lines.lock()),
        }
    }
}

pub enum LogLine {
    Info(String),
    Warn(String),
    Error(String),
}

impl std::fmt::Display for LogLine {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            LogLine::Info(s) => write!(f, "INFO: {s}"),
            LogLine::Warn(s) => write!(f, "WARN: {s}"),
            LogLine::Error(s) => write!(f, "ERROR: {s}"),
        }
    }
}

impl std::fmt::Debug for LogLine {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        std::fmt::Display::fmt(self, f)
    }
}

#[pymethods]
impl PyPluginCallApi {
    #[pyo3(signature = (*args))]
    fn info(&self, py: Python<'_>, args: &Bound<'_, PyTuple>) -> PyResult<()> {
        let line = self.log_args_to_string(args)?;
        let logger = Arc::clone(&self.logger);
        py.detach(|| {
            info!("processing engine: {}", line);
            logger.log(LogLevel::Info, line);
        });
        Ok(())
    }

    #[pyo3(signature = (*args))]
    fn warn(&self, py: Python<'_>, args: &Bound<'_, PyTuple>) -> PyResult<()> {
        let line = self.log_args_to_string(args)?;
        let logger = Arc::clone(&self.logger);
        py.detach(|| {
            warn!("processing engine: {}", line);
            logger.log(LogLevel::Warn, line);
        });
        Ok(())
    }

    #[pyo3(signature = (*args))]
    fn error(&self, py: Python<'_>, args: &Bound<'_, PyTuple>) -> PyResult<()> {
        let line = self.log_args_to_string(args)?;
        let logger = Arc::clone(&self.logger);
        py.detach(|| {
            error!("processing engine: {}", line);
            logger.log(LogLevel::Error, line);
        });
        Ok(())
    }

    /// Write line protocol to the database this trigger is attached to.
    ///
    /// Legacy api that batches writes and writes them at the end of plugin execution.
    fn write(&self, line_builder: &Bound<'_, PyAny>) -> PyResult<()> {
        let line = line_builder.getattr("build")?.call0()?;
        let line_str = line.extract::<String>()?;

        self.write_accumulator
            .lock()
            .write_back_lines
            .push(line_str);

        Ok(())
    }

    /// Write line protocol to a specified database.
    ///
    /// Legacy api that batches writes and writes them at the end of plugin execution.
    fn write_to_db(&self, db_name: &str, line_builder: &Bound<'_, PyAny>) -> PyResult<()> {
        let line = line_builder.getattr("build")?.call0()?;
        let line_str = line.extract::<String>()?;

        self.write_accumulator
            .lock()
            .write_db_lines
            .entry(db_name.to_string())
            .or_default()
            .push(line_str);

        Ok(())
    }

    /// Write line protocol to the database this trigger is attached to.
    ///
    /// Writes synchronously via the write buffer. Use `write_sync_to_db` to write to a different database.
    fn write_sync(
        &self,
        py: Python<'_>,
        line_builder: &Bound<'_, PyAny>,
        no_sync: bool,
    ) -> PyResult<()> {
        self.write_sync_to_db(py, &self.db_schema.name, line_builder, no_sync)
    }

    /// Write line protocol to a specified database.
    ///
    /// Writes synchronously via the write buffer. Use `write` to write to the trigger's database.
    ///
    /// # Panics
    ///
    /// Uses `tokio::task::block_in_place` which requires a multi-threaded tokio runtime.
    fn write_sync_to_db(
        &self,
        py: Python<'_>,
        db_name: &str,
        line_builder: &Bound<'_, PyAny>,
        no_sync: bool,
    ) -> PyResult<()> {
        let line = line_builder.getattr("build")?.call0()?;
        let line_str = line.extract::<String>()?;

        let write_buffer = Arc::clone(&self.write_buffer);
        let db_name = db_name.to_string();

        let ingest_time_nanos = SystemTime::now()
            .duration_since(SystemTime::UNIX_EPOCH)
            .unwrap()
            .as_nanos() as i64;

        py.detach(|| {
            tokio::task::block_in_place(|| {
                let namespace = NamespaceName::new(db_name.clone())
                    .map_err(|e| format!("invalid database name: {e}"))?;
                let ingest_time = Time::from_timestamp_nanos(ingest_time_nanos);

                tokio::runtime::Handle::current()
                    .block_on(write_buffer.write_lp(
                        namespace,
                        &line_str,
                        ingest_time,
                        false,
                        Precision::Nanosecond,
                        no_sync,
                    ))
                    .map(|_| ())
                    .map_err(|e| format!("{e}"))
            })
        })
        .map_err(|e| PyException::new_err(format!("error writing line protocol to {db_name}: {e}")))
    }

    #[pyo3(signature = (query, args=None))]
    fn query(
        &self,
        py: Python<'_>,
        query: String,
        args: Option<std::collections::HashMap<String, String>>,
    ) -> PyResult<Py<PyList>> {
        let query_executor = Arc::clone(&self.query_executor);
        let db_schema_name = Arc::clone(&self.db_schema.name);

        let params = args.map(|args| {
            let mut params = StatementParams::new();
            for (key, value) in args {
                params.insert(key, value);
            }
            params
        });

        let batches = py.detach(|| {
            tokio::task::block_in_place(|| {
                tokio::runtime::Handle::current().block_on(async {
                    let res = query_executor
                        .query_sql(db_schema_name.as_ref(), &query, params, None, None)
                        .await
                        .map_err(|e| format!("error: {e} executing query: {query}"))?;

                    res.try_collect::<Vec<RecordBatch>>()
                        .await
                        .map_err(|e| format!("error: {e} executing query: {query}"))
                })
            })
        });

        let batches = batches.map_err(QueryError::new_err)?;

        // Pre-create Python strings for field/tag names once for all batches;
        // schema must be the same across batches.
        let Some(first_batch) = batches.first() else {
            return Ok(PyList::empty(py).unbind());
        };
        let field_names: Vec<Bound<'_, PyString>> = first_batch
            .schema()
            .fields()
            .iter()
            .map(|f| PyString::new(py, f.name().as_str()))
            .collect();

        let total_rows: usize = batches.iter().map(|b| b.num_rows()).sum();
        let mut rows: Vec<Py<PyAny>> = Vec::with_capacity(total_rows);

        for batch in &batches {
            debug_assert_eq!(
                // datafusion should ensure all batches have the same schema; check it in debug builds
                batch.num_columns(),
                field_names.len(),
                "unexpected batch schema mismatch: expected {} columns, got {}",
                field_names.len(),
                batch.num_columns()
            );
            let num_rows = batch.num_rows();
            for row_idx in 0..num_rows {
                let row = PyDict::new(py);
                for (col_idx, field_name) in field_names.iter().enumerate() {
                    let array = batch.column(col_idx);

                    if array.is_null(row_idx) {
                        row.set_item(field_name, py.None())?;
                        continue;
                    }

                    match array.data_type() {
                        DataType::Int64 => {
                            let array = array.as_any().downcast_ref::<Int64Array>().unwrap();
                            row.set_item(field_name, array.value(row_idx))?;
                        }
                        DataType::UInt64 => {
                            let array = array.as_any().downcast_ref::<UInt64Array>().unwrap();
                            row.set_item(field_name, array.value(row_idx))?;
                        }
                        DataType::Float64 => {
                            let array = array.as_any().downcast_ref::<Float64Array>().unwrap();
                            row.set_item(field_name, array.value(row_idx))?;
                        }
                        DataType::Utf8 => {
                            let array = array.as_any().downcast_ref::<StringArray>().unwrap();
                            row.set_item(field_name, array.value(row_idx))?;
                        }
                        DataType::Boolean => {
                            let array = array.as_any().downcast_ref::<BooleanArray>().unwrap();
                            row.set_item(field_name, array.value(row_idx))?;
                        }
                        DataType::Timestamp(_, _) => {
                            let array = array
                                .as_any()
                                .downcast_ref::<TimestampNanosecondArray>()
                                .unwrap();
                            row.set_item(field_name, array.value(row_idx))?;
                        }
                        DataType::Dictionary(_, _) => {
                            let col = array
                                .as_any()
                                .downcast_ref::<DictionaryArray<Int32Type>>()
                                .expect("unexpected datatype");
                            let values = col.values();
                            let values = values
                                .as_any()
                                .downcast_ref::<StringArray>()
                                .expect("unexpected datatype");
                            let key_idx = col.keys().value(row_idx) as usize;
                            let val = values.value(key_idx).to_string();
                            row.set_item(field_name, val)?;
                        }
                        _ => {
                            return Err(PyValueError::new_err(format!(
                                "Unsupported data type: {:?}",
                                array.data_type()
                            )));
                        }
                    }
                }
                rows.push(row.into());
            }
        }

        let list = PyList::new(py, rows)?.unbind();
        Ok(list)
    }

    #[getter]
    fn cache(&self, py: Python<'_>) -> PyResult<PyCache> {
        let py_cache = self.py_cache.clone();
        let cache_store = Arc::clone(&py_cache.cache_store);
        py.detach(|| {
            cache_store.lock().cleanup();
        });
        Ok(py_cache)
    }
}

// no #[pymethods] here so these methods are available to rust code but not python code.
impl PyPluginCallApi {
    fn log_args_to_string(&self, args: &Bound<'_, PyTuple>) -> PyResult<String> {
        let line = args
            .try_iter()?
            .map(|arg| arg?.str()?.extract::<String>())
            .collect::<Result<Vec<String>, _>>()?
            .join(" ");
        Ok(line)
    }
}

// constant for the process writes call site string
const PROCESS_WRITES_CALL_SITE: &str = "process_writes";

const PROCESS_SCHEDULED_CALL_SITE: &str = "process_scheduled_call";

const PROCESS_REQUEST_CALL_SITE: &str = "process_request";

use crate::py_conversion::ToPythonTableBatches;

const LINE_BUILDER_CODE: &str = r#"
from typing import Optional
from collections import OrderedDict

class InfluxDBError(Exception):
    """Base exception for InfluxDB-related errors"""
    pass

class InvalidMeasurementError(InfluxDBError):
    """Raised when measurement name is invalid"""
    pass

class InvalidKeyError(InfluxDBError):
    """Raised when a tag or field key is invalid"""
    pass

class InvalidLineError(InfluxDBError):
    """Raised when a line protocol string is invalid"""
    pass

class LineBuilder:
    def __init__(self, measurement: str):
        if ' ' in measurement:
            raise InvalidMeasurementError("Measurement name cannot contain spaces")
        self.measurement = measurement
        self.tags: OrderedDict[str, str] = OrderedDict()
        self.fields: OrderedDict[str, str] = OrderedDict()
        self._timestamp_ns: Optional[int] = None

    def _validate_key(self, key: str, key_type: str) -> None:
        """Validate that a key does not contain spaces, commas, or equals signs."""
        if not key:
            raise InvalidKeyError(f"{key_type} key cannot be empty")
        if ' ' in key:
            raise InvalidKeyError(f"{key_type} key '{key}' cannot contain spaces")
        if ',' in key:
            raise InvalidKeyError(f"{key_type} key '{key}' cannot contain commas")
        if '=' in key:
            raise InvalidKeyError(f"{key_type} key '{key}' cannot contain equals signs")

    def _escape_measurement(self, value: str) -> str:
        """Escape characters in measurement names according to line protocol."""
        return value.replace(',', '\\,').replace(' ', '\\ ')

    def _escape_tag_value(self, value: str) -> str:
        """Escape characters in tag values according to line protocol."""
        return value.replace('\\', '\\\\').replace(',', '\\,').replace('=', '\\=').replace(' ', '\\ ')

    def _escape_field_key(self, value: str) -> str:
        """Escape characters in field keys according to line protocol."""
        return value.replace('\\', '\\\\').replace(',', '\\,').replace('=', '\\=').replace(' ', '\\ ')

    def tag(self, key: str, value: str) -> 'LineBuilder':
        """Add a tag to the line protocol."""
        self._validate_key(key, "tag")
        self.tags[key] = str(value)
        return self

    def uint64_field(self, key: str, value: int) -> 'LineBuilder':
        """Add an unsigned integer field to the line protocol."""
        self._validate_key(key, "field")
        if value < 0:
            raise ValueError(f"uint64 field '{key}' cannot be negative")
        self.fields[key] = f"{value}u"
        return self

    def int64_field(self, key: str, value: int) -> 'LineBuilder':
        """Add an integer field to the line protocol."""
        self._validate_key(key, "field")
        self.fields[key] = f"{value}i"
        return self

    def float64_field(self, key: str, value: float) -> 'LineBuilder':
        """Add a float field to the line protocol."""
        self._validate_key(key, "field")
        # Check if value has no decimal component
        self.fields[key] = f"{int(value)}.0" if value % 1 == 0 else str(value)
        return self

    def string_field(self, key: str, value: str) -> 'LineBuilder':
        """Add a string field to the line protocol."""
        self._validate_key(key, "field")
        # Escape quotes and backslashes in string values
        escaped_value = value.replace('\\', '\\\\').replace('"', '\\"')
        self.fields[key] = f'"{escaped_value}"'
        return self

    def bool_field(self, key: str, value: bool) -> 'LineBuilder':
        """Add a boolean field to the line protocol."""
        self._validate_key(key, "field")
        self.fields[key] = 't' if value else 'f'
        return self

    def time_ns(self, timestamp_ns: int) -> 'LineBuilder':
        """Set the timestamp in nanoseconds."""
        self._timestamp_ns = timestamp_ns
        return self

    def build(self) -> str:
        """Build the line protocol string."""
        # Start with measurement name (escape commas and spaces)
        line = self._escape_measurement(self.measurement)

        # Add tags if present
        if self.tags:
            tags_str = ','.join(
                f"{key}={self._escape_tag_value(value)}"
                for key, value in self.tags.items()
            )
            line += f",{tags_str}"

        # Add fields (required)
        if not self.fields:
            raise InvalidLineError(f"At least one field is required: {line}")

        fields_str = ','.join(
            f"{self._escape_field_key(key)}={value}"
            for key, value in self.fields.items()
        )
        line += f" {fields_str}"

        # Add timestamp if present
        if self._timestamp_ns is not None:
            line += f" {self._timestamp_ns}"

        return line"#;

fn args_to_py_object<'py>(
    py: Python<'py>,
    args: &Option<HashMap<String, String>>,
) -> Option<Bound<'py, PyDict>> {
    args.as_ref().map(|args| map_to_py_object(py, args))
}

fn map_to_py_object<'py>(py: Python<'py>, map: &HashMap<String, String>) -> Bound<'py, PyDict> {
    let dict = PyDict::new(py);
    for (key, value) in map {
        dict.set_item(key, value).unwrap();
    }
    dict
}

/// Sets up the LineBuilder class in Python's environment for access by all plugins.
///
/// Executes the LineBuilder code in `__main__` and adds it to builtins so it's accessible
/// from both single-file plugins and multi-file plugins.
fn setup_line_builder(py: Python<'_>) -> Result<(), anyhow::Error> {
    // specifically use globals = None which defaults to __main__ so LineBuilder is
    // accessible to all plugins
    py.run(&CString::new(LINE_BUILDER_CODE).unwrap(), None, None)
        .map_err(|e| anyhow::Error::new(e).context("failed to eval the LineBuilder API code"))?;

    // Make LineBuilder available to all modules by adding it to builtins.
    // This ensures multi-file plugins can access it without explicit imports.
    let builtins = py.import("builtins")?;
    let main_module = py.import("__main__")?;
    let line_builder = main_module.getattr("LineBuilder")?;
    builtins.setattr("LineBuilder", line_builder)?;

    Ok(())
}

/// Loads a Python function from either a multi-file plugin module or single-file plugin code.
///
/// For multi-file plugins (when `plugin_root` is `Some`), imports the module and retrieves
/// the function. For single-file plugins, executes the code in an isolated namespace and
/// retrieves the function.
fn load_plugin_function<'py>(
    py: Python<'py>,
    code: &str,
    plugin_root: Option<&Path>,
    call_site: &str,
    missing_fn_error: ExecutePluginError,
) -> Result<Bound<'py, PyAny>, ExecutePluginError> {
    if let Some(root_path) = plugin_root {
        load_function_from_module(py, root_path, call_site).map_err(|_| missing_fn_error)
    } else {
        // Create isolated globals for this plugin execution. Without this, single-file
        // plugins would share __main__'s namespace and could overwrite each other's
        // function definitions during concurrent execution.
        let globals = PyDict::new(py);
        py.run(&CString::new(code).unwrap(), Some(&globals), None)
            .map_err(anyhow::Error::from)?;
        py.eval(&CString::new(call_site).unwrap(), Some(&globals), None)
            .map_err(|_| missing_fn_error)
    }
}

/// Loads a Python function from a multi-file plugin module.
///
/// This function temporarily adds the plugin's parent directory to Python's sys.path,
/// imports the module, retrieves the function, and then cleans up sys.path.
fn load_function_from_module<'py>(
    py: Python<'py>,
    plugin_root: &Path,
    function_name: &str,
) -> Result<Bound<'py, PyAny>, anyhow::Error> {
    let parent_dir = plugin_root
        .parent()
        .ok_or_else(|| anyhow!("Plugin root has no parent directory"))?;
    let module_name = plugin_root
        .file_name()
        .and_then(|n| n.to_str())
        .ok_or_else(|| anyhow!("Invalid plugin directory name"))?;

    let sys = py.import("sys")?;
    let sys_path = sys.getattr("path")?;

    let parent_dir_str = parent_dir
        .to_str()
        .ok_or_else(|| anyhow!("Invalid UTF-8 in parent directory path"))?;

    // Add parent directory to sys.path
    sys_path.call_method1("insert", (0, parent_dir_str))?;

    // Attempt to import the module and get the function
    // Cleanup happens regardless of success or failure
    let import_result = py
        .import(module_name)
        .and_then(|module| module.getattr(function_name));

    // Always cleanup sys.path, even if import failed
    // Ignore cleanup errors as they don't affect the import result
    let _ = sys_path.call_method1("pop", (0,));

    // Convert PyErr to anyhow error with helpful context
    import_result.map_err(|e| {
        // Check if error is about missing function or import failure
        let error_str = e.to_string();
        if error_str.contains("has no attribute") {
            anyhow!(
                "Plugin module '{}' does not contain function '{}'. \
                 Multi-file plugins must define this function in __init__.py",
                module_name,
                function_name
            )
        } else {
            anyhow!(
                "Failed to import plugin module '{}': {}\n\
                 Hint: Check for syntax errors or missing dependencies in Python files.",
                module_name,
                e
            )
        }
    })
}

/// Execute a WAL flush trigger plugin with data that implements `ToPythonTableBatches`.
#[allow(clippy::too_many_arguments)]
pub fn execute_wal_flush_trigger<T: ToPythonTableBatches>(
    code: &str,
    wal_data: &T,
    schema: Arc<DatabaseSchema>,
    query_executor: Arc<dyn QueryExecutor>,
    write_buffer: Arc<dyn Bufferer>,
    logger: PluginLogger,
    table_filter: Option<TableId>,
    args: &Option<HashMap<String, String>>,
    py_cache: PyCache,
    plugin_root: Option<&Path>,
) -> Result<PluginReturnState, ExecutePluginError> {
    logger.log(
        LogLevel::Info,
        String::from("starting execution of wal plugin."),
    );
    let start_time = Instant::now();

    let logger = Arc::new(logger);
    let write_accumulator: Arc<Mutex<WriteAccumulator>> = Default::default();

    Python::attach(|py| {
        setup_line_builder(py)?;
        let py_batches = wal_data.to_python_table_batches(py, &schema, table_filter)?;
        let local_api = PyPluginCallApi {
            db_schema: schema,
            query_executor,
            write_buffer,
            write_accumulator: Arc::clone(&write_accumulator),
            logger: Arc::clone(&logger),
            py_cache,
        }
        .into_pyobject(py)
        .map_err(anyhow::Error::from)?;

        let args = args_to_py_object(py, args);

        let py_func = load_plugin_function(
            py,
            code,
            plugin_root,
            PROCESS_WRITES_CALL_SITE,
            ExecutePluginError::MissingProcessWritesFunction,
        )?;

        py_func
            .call1((local_api, py_batches.unbind(), args))
            .map_err(anyhow::Error::from)?;

        Ok::<(), ExecutePluginError>(())
    })?;

    logger.log(
        LogLevel::Info,
        format!(
            "finished execution in {}",
            format_duration(start_time.elapsed())
        ),
    );

    let writes = std::mem::take(&mut *write_accumulator.lock());
    Ok(PluginReturnState {
        log_lines: logger.take_log_lines(),
        write_back_lines: writes.write_back_lines,
        write_db_lines: writes.write_db_lines,
    })
}

#[allow(clippy::too_many_arguments)]
pub fn execute_schedule_trigger(
    code: &str,
    schedule_time: DateTime<Utc>,
    schema: Arc<DatabaseSchema>,
    query_executor: Arc<dyn QueryExecutor>,
    write_buffer: Arc<dyn Bufferer>,
    logger: PluginLogger,
    args: &Option<HashMap<String, String>>,
    py_cache: PyCache,
    plugin_root: Option<&Path>,
) -> Result<PluginReturnState, ExecutePluginError> {
    logger.log(
        LogLevel::Info,
        format!("starting execution with scheduled time {schedule_time}"),
    );
    let start_time = Instant::now();

    let logger = Arc::new(logger);
    let write_accumulator: Arc<Mutex<WriteAccumulator>> = Default::default();

    Python::attach(|py| {
        setup_line_builder(py)?;
        let local_api = PyPluginCallApi {
            db_schema: schema,
            query_executor,
            write_buffer,
            write_accumulator: Arc::clone(&write_accumulator),
            logger: Arc::clone(&logger),
            py_cache,
        }
        .into_pyobject(py)
        .map_err(anyhow::Error::from)?;

        let py_datetime = PyDateTime::from_timestamp(py, schedule_time.timestamp() as f64, None)
            .map_err(|e| {
                anyhow::Error::new(e).context(format!(
                    "error converting the schedule time {schedule_time} to Python time: \
                likely python build or libpython issue"
                ))
            })?;

        // turn args into an optional dict to pass into python
        let args = args_to_py_object(py, args);

        let py_func = load_plugin_function(
            py,
            code,
            plugin_root,
            PROCESS_SCHEDULED_CALL_SITE,
            ExecutePluginError::MissingProcessScheduledCallFunction,
        )?;

        py_func
            .call1((local_api, py_datetime, args))
            .map_err(anyhow::Error::from)?;

        Ok::<(), ExecutePluginError>(())
    })?;

    logger.log(
        LogLevel::Info,
        format!(
            "finished execution in {}",
            format_duration(start_time.elapsed())
        ),
    );

    let writes = std::mem::take(&mut *write_accumulator.lock());
    Ok(PluginReturnState {
        log_lines: logger.take_log_lines(),
        write_back_lines: writes.write_back_lines,
        write_db_lines: writes.write_db_lines,
    })
}

#[allow(clippy::too_many_arguments, clippy::type_complexity)]
pub fn execute_request_trigger(
    code: &str,
    db_schema: Arc<DatabaseSchema>,
    query_executor: Arc<dyn QueryExecutor>,
    write_buffer: Arc<dyn Bufferer>,
    logger: PluginLogger,
    args: &Option<HashMap<String, String>>,
    query_params: HashMap<String, String>,
    request_headers: HashMap<String, String>,
    request_body: Bytes,
    py_cache: PyCache,
    plugin_root: Option<&Path>,
) -> Result<(u16, HashMap<String, String>, String, PluginReturnState), ExecutePluginError> {
    logger.log(
        LogLevel::Info,
        String::from("starting execution of request plugin."),
    );
    let start_time = Instant::now();

    let logger = Arc::new(logger);
    let write_accumulator: Arc<Mutex<WriteAccumulator>> = Default::default();

    let (response_code, response_headers, response_body) = Python::attach(|py| {
        setup_line_builder(py)?;
        let local_api = PyPluginCallApi {
            db_schema,
            query_executor,
            write_buffer,
            write_accumulator: Arc::clone(&write_accumulator),
            logger: Arc::clone(&logger),
            py_cache,
        }
        .into_pyobject(py)
        .map_err(anyhow::Error::from)?;

        // turn args into an optional dict to pass into python
        let args = args_to_py_object(py, args);

        let query_params = map_to_py_object(py, &query_params);
        let request_params = map_to_py_object(py, &request_headers);

        let py_func = load_plugin_function(
            py,
            code,
            plugin_root,
            PROCESS_REQUEST_CALL_SITE,
            ExecutePluginError::MissingProcessRequestFunction,
        )?;

        // convert the body bytes into python bytes blob
        let request_body = PyBytes::new(py, &request_body[..]);

        // get the result from calling the python function
        let result = py_func
            .call1((local_api, query_params, request_params, request_body, args))
            .map_err(|e| anyhow!("Python function call failed: {}", e))?;

        // Process the result according to Flask conventions
        process_flask_response(py, result)
    })?;

    logger.log(
        LogLevel::Info,
        format!(
            "finished execution in {}",
            format_duration(start_time.elapsed())
        ),
    );

    let writes = std::mem::take(&mut *write_accumulator.lock());
    let plugin_state = PluginReturnState {
        log_lines: logger.take_log_lines(),
        write_back_lines: writes.write_back_lines,
        write_db_lines: writes.write_db_lines,
    };

    Ok((response_code, response_headers, response_body, plugin_state))
}

fn process_flask_response(
    py: Python<'_>,
    result: Bound<'_, PyAny>,
) -> Result<(u16, HashMap<String, String>, String), anyhow::Error> {
    let default_status: u16 = 200;
    let default_headers: HashMap<String, String> = HashMap::new();
    let default_mimetype = "text/html";

    // Check if it's a Flask Response object
    if let Ok(true) = result
        .call_method0("__flask_response__")
        .and_then(|r| r.extract::<bool>())
    {
        // It's a Flask Response object, extract status, headers, and body
        let status: u16 = result.getattr("status_code")?.extract()?;
        let body: String = result.call_method0("get_data")?.extract()?;

        // Extract headers
        let headers_dict = result.getattr("headers")?;
        let headers: std::collections::HashMap<String, String> = headers_dict.extract()?;

        return Ok((status, headers.into_iter().collect(), body));
    }

    // Check if it's a tuple
    if let Ok(tuple) = result.cast::<PyTuple>() {
        let tuple_len = tuple.len()?;
        if tuple_len > 0 && tuple_len <= 3 {
            // Extract response part (first element)
            let response = tuple.get_item(0)?;
            let (response_body, mut headers) = process_response_part(py, response)?;

            let mut status = default_status;

            // Handle status and/or headers if provided
            if tuple_len >= 2 {
                let second = tuple.get_item(1)?;

                // Check if the second item is a status code or headers
                if let Ok(status_code) = second.extract::<i64>() {
                    status = status_code.try_into().unwrap_or(500);
                } else if let Ok(header_dict) = second.cast::<PyDict>() {
                    // It's headers
                    let additional_headers: std::collections::HashMap<String, String> =
                        header_dict.extract()?;
                    headers.extend(additional_headers);
                }

                // If there's a third item, it must be headers
                if tuple_len == 3 {
                    let header_item = tuple.get_item(2)?;
                    let Ok(header_dict) = header_item.cast::<PyDict>() else {
                        bail!("expected a dictionary in header item");
                    };
                    let additional_headers: std::collections::HashMap<String, String> =
                        header_dict.extract()?;
                    headers.extend(additional_headers);
                }
            }

            return Ok((status, headers, response_body));
        }
    }

    // Check if it's a string
    if let Ok(string_val) = result.extract::<String>() {
        let mut headers = default_headers.clone();
        headers.insert("Content-Type".to_string(), default_mimetype.to_string());
        return Ok((default_status, headers, string_val));
    }

    // Check if it's a dict or list for JSON response
    if result.is_instance_of::<PyDict>() || result.is_instance_of::<PyList>() {
        // We need to jsonify this
        let json_module = py.import("json")?;
        let json_string = json_module.call_method1("dumps", (result,))?;
        let response_body: String = json_string.extract()?;

        let mut headers = default_headers.clone();
        headers.insert("Content-Type".to_string(), "application/json".to_string());

        return Ok((default_status, headers, response_body));
    }

    // Check if it's an iterator or generator returning strings or bytes
    if let Ok(true) = result.hasattr("__iter__") {
        // Handling streaming response would require adapting the return type
        // For now, we'll convert it to a concatenated string
        let mut stream_content = String::new();

        let iter = result.try_iter()?;
        for item in iter {
            let item = item?;
            if let Ok(s) = item.extract::<String>() {
                stream_content.push_str(&s);
            } else if let Ok(b) = item.extract::<&[u8]>() {
                // Convert bytes to string - might need better UTF-8 handling
                if let Ok(s) = std::str::from_utf8(b) {
                    stream_content.push_str(s);
                }
            }
        }

        let mut headers = default_headers.clone();
        headers.insert("Content-Type".to_string(), default_mimetype.to_string());

        return Ok((default_status, headers, stream_content));
    }

    // If we can't identify the response type, return an error
    Err(anyhow!("Unsupported return type from Python function"))
}

fn process_response_part(
    py: Python<'_>,
    response: Bound<'_, PyAny>,
) -> Result<(String, HashMap<String, String>), anyhow::Error> {
    let mut headers = HashMap::new();

    // If it's a string, return it directly
    if let Ok(string_val) = response.extract::<String>() {
        headers.insert("Content-Type".to_string(), "text/html".to_string());
        return Ok((string_val, headers));
    }

    // If it's a dict or list, convert to JSON
    if response.is_instance_of::<PyDict>() || response.is_instance_of::<PyList>() {
        let json_module = py.import("json")?;
        let json_string = json_module.call_method1("dumps", (response,))?;
        let response_body: String = json_string.extract()?;

        headers.insert("Content-Type".to_string(), "application/json".to_string());
        return Ok((response_body, headers));
    }

    // Default fallback
    let response_str = response.str()?.extract::<String>()?;
    headers.insert("Content-Type".to_string(), "text/plain".to_string());

    Ok((response_str, headers))
}

// Cache entry with optional expiration
#[derive(Debug)]
pub struct CacheEntry {
    value: Py<PyAny>,         // Python object reference with proper reference counting
    expires_at: Option<Time>, // Expiration time if any
}

impl CacheEntry {
    fn new(value: Py<PyAny>, ttl: Option<f64>, time_provider: Arc<dyn TimeProvider>) -> Self {
        let expires_at = ttl.map(|seconds| time_provider.now() + Duration::from_secs_f64(seconds));
        Self { value, expires_at }
    }
}

// Cache store that manages namespaced caches
#[derive(Debug)]
pub struct CacheStore {
    // Map of namespace -> (key -> cache entry)
    namespaces: HashMap<CacheId, ExpiringCache>,
    time_provider: Arc<dyn TimeProvider>,
    last_cleanup: Time,
    cleanup_interval: Duration,
}

#[derive(Debug)]
pub struct ExpiringCache {
    entries: HashMap<String, CacheEntry>,
    expirations: BTreeMap<Time, HashSet<String>>,
    default_ttl: Option<Duration>,
    time_provider: Arc<dyn TimeProvider>,
}

impl ExpiringCache {
    fn new(time_provider: Arc<dyn TimeProvider>, default_ttl: Option<Duration>) -> Self {
        Self {
            entries: HashMap::default(),
            expirations: BTreeMap::default(),
            default_ttl,
            time_provider,
        }
    }

    fn insert(&mut self, key: String, mut entry: CacheEntry) {
        // this is for test call caches, which should expire.
        if self.default_ttl.is_some() && entry.expires_at.is_none() {
            entry.expires_at = Some(self.time_provider.now() + self.default_ttl.unwrap());
        }
        // if key has a ttl, record its expiration.
        let expiration = entry.expires_at;
        // if this was an overwrite, remove from expirations
        if let Some(old_entry) = self.entries.insert(key.clone(), entry)
            && let Some(old_expiration) = old_entry.expires_at
        {
            self.expirations
                .get_mut(&old_expiration)
                .unwrap()
                .remove(&key);
        }
        if let Some(expiration) = expiration {
            self.expirations.entry(expiration).or_default().insert(key);
        }
    }

    fn get(&mut self, key: &str, py: Python<'_>) -> Option<Py<PyAny>> {
        let entry = self.entries.get(key)?;
        if let Some(expiration) = entry.expires_at
            && expiration <= self.time_provider.now()
        {
            self.remove(key);
            return None;
        }
        Some(entry.value.clone_ref(py))
    }

    fn remove(&mut self, key: &str) -> bool {
        if let Some(entry) = self.entries.remove(key) {
            if let Some(expiration) = entry.expires_at {
                self.expirations.get_mut(&expiration).unwrap().remove(key);
            }
            return true;
        }
        false
    }

    fn cleanup(&mut self) {
        let now = self.time_provider.now();
        if self.expirations.is_empty() || *self.expirations.first_key_value().unwrap().0 > now {
            return;
        }
        let mut unexpired_expirations = self.expirations.split_off(&now);
        for (_, keys) in self.expirations.iter() {
            for key in keys {
                self.entries.remove(key);
            }
        }
        std::mem::swap(&mut self.expirations, &mut unexpired_expirations);
    }

    fn is_empty(&self) -> bool {
        self.entries.is_empty()
    }
}

#[pyclass]
#[derive(Hash, Eq, PartialEq, Debug, Clone)]
enum CacheId {
    Global(),
    GlobalTest(String),
    Trigger {
        database: String,
        trigger_name: String,
    },
    TriggerTest(String),
}

impl CacheId {
    fn default_expiration(&self) -> Option<Duration> {
        match self {
            Global() | Trigger { .. } => None,
            // for tests, keep values around for 30 minutes.
            GlobalTest(_) | TriggerTest(_) => Some(Duration::from_secs(30 * 60)),
        }
    }
}

impl CacheStore {
    pub fn new(time_provider: Arc<dyn TimeProvider>, cleanup_interval: Duration) -> Self {
        let last_cleanup = time_provider.now();
        Self {
            namespaces: HashMap::new(),
            time_provider,
            cleanup_interval,
            last_cleanup,
        }
    }

    fn should_run_cleanup(&self) -> bool {
        self.time_provider
            .now()
            .checked_duration_since(self.last_cleanup)
            .unwrap_or_default()
            >= self.cleanup_interval
    }

    fn cleanup(&mut self) {
        if !self.should_run_cleanup() {
            return;
        }
        for cache in self.namespaces.values_mut() {
            cache.cleanup();
        }

        self.namespaces.retain(|_, cache| !cache.is_empty());
    }

    fn put(&mut self, cache_id: &CacheId, key: &str, value: Py<PyAny>, ttl: Option<f64>) {
        let entry = CacheEntry::new(value, ttl, Arc::clone(&self.time_provider));
        if let Some(cache) = self.namespaces.get_mut(cache_id) {
            cache.insert(key.to_string(), entry);
            return;
        }
        self.namespaces
            .entry(cache_id.clone())
            .or_insert_with(|| {
                ExpiringCache::new(
                    Arc::clone(&self.time_provider),
                    cache_id.default_expiration(),
                )
            })
            .insert(key.to_string(), entry);
    }

    fn get(&mut self, cache_id: &CacheId, py: Python<'_>, key: &str) -> Option<Py<PyAny>> {
        let cache = self.namespaces.get_mut(cache_id)?;

        cache.get(key, py)
    }

    fn delete(&mut self, cache_id: &CacheId, key: &str) -> bool {
        if let Some(cache) = self.namespaces.get_mut(cache_id) {
            cache.remove(key)
        } else {
            false
        }
    }
    pub fn drop_trigger_cache(&mut self, database: String, trigger_name: String) -> bool {
        self.namespaces
            .remove(&Trigger {
                database,
                trigger_name,
            })
            .is_some()
    }
}

// Python class for Cache
#[pyclass]
#[derive(Debug, Clone)]
pub struct PyCache {
    global_cache_id: CacheId,
    local_cache_id: CacheId,
    cache_store: Arc<Mutex<CacheStore>>,
}

impl PyCache {
    fn cache_id(&self, use_global: Option<bool>) -> &CacheId {
        if use_global.unwrap_or_default() {
            &self.global_cache_id
        } else {
            &self.local_cache_id
        }
    }
    pub fn new_test_cache(cache_store: Arc<Mutex<CacheStore>>, test_name: String) -> Self {
        Self {
            global_cache_id: GlobalTest(test_name.clone()),
            local_cache_id: TriggerTest(test_name),
            cache_store,
        }
    }

    pub fn new_trigger_cache(
        cache_store: Arc<Mutex<CacheStore>>,
        database: String,
        trigger_name: String,
    ) -> Self {
        Self {
            global_cache_id: Global(),
            local_cache_id: Trigger {
                database,
                trigger_name,
            },
            cache_store,
        }
    }
}

#[pymethods]
impl PyCache {
    #[pyo3(signature = (key, value, ttl=None, use_global=None))]
    fn put(
        &self,
        key: String,
        value: Py<PyAny>,
        ttl: Option<f64>,
        use_global: Option<bool>,
    ) -> PyResult<()> {
        let cache_id = self.cache_id(use_global);
        self.cache_store.lock().put(cache_id, &key, value, ttl);

        Ok(())
    }

    #[pyo3(signature = (key, default=None, use_global=None))]
    fn get(
        &self,
        key: String,
        default: Option<Py<PyAny>>,
        use_global: Option<bool>,
    ) -> PyResult<Py<PyAny>> {
        Python::attach(|py| {
            let cache_id = self.cache_id(use_global);
            let result = self.cache_store.lock().get(cache_id, py, &key);

            match result {
                Some(value) => {
                    // Return the retrieved Python object
                    Ok(value)
                }
                None => {
                    // Return the default value if provided
                    if let Some(default_value) = default {
                        Ok(default_value)
                    } else {
                        // Return None if no default was provided
                        let x = py.None();
                        Ok(x)
                    }
                }
            }
        })
    }

    #[pyo3(signature = (key, use_global=None))]
    fn delete(&self, key: String, use_global: Option<bool>) -> PyResult<bool> {
        let cache_id = self.cache_id(use_global);
        let result = self.cache_store.lock().delete(cache_id, &key);

        Ok(result)
    }
}

// Module initialization
#[pymodule]
fn influxdb3_py_api(_m: &Bound<'_, PyModule>) -> PyResult<()> {
    Ok(())
}
